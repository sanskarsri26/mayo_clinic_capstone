{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fbff6d-e3ee-4c91-be29-d6441e345a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I 2025-11-20 07:35:15,051] Trial 93 finished with value: 0.7909400798921143 and parameters: {'backbone_lr': 0.00011984014753946137, 'head_lr': 5.693955800220594e-05, 'weight_decay': 1.7206999596641163e-08, 'optimizer_name': 'adamw', 'loss_name': 'softmargin', 'activation_name': 'gelu', 'head_hidden_dim': 512, 'head_dropout': 0.3929098112015572, 'tune_epochs': 16}. Best is trial 72 with value: 0.7978573596844559."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24d4a4-3b19-4339-86b2-882338cf0b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] New run id: 20251124-163525\n",
      "[run] Output directory: ./runs_adapt/20251124-163525\n",
      "[run] Saved config.json\n",
      "\n",
      "[data] Loading training CSV: /scratch/ssriva94/Capstone/new/low-quality-image.fixed_paths.csv\n",
      "[data] Total rows (after patient_id ensure): 1406\n",
      "[data] Unique patients (total): 1366\n",
      "[data] Train rows: 1130, Val rows: 276\n",
      "[data] Unique patients (train): 1092\n",
      "[data] Unique patients (val):   274\n",
      "\n",
      "[sampler] Building inverse-frequency sampler from /scratch/ssriva94/Capstone/new/low-quality-image.csv\n",
      "[sampler] Detected path column: New_path\n",
      "[sampler] Group counts in train:\n",
      "          other            -> 155\n",
      "          white_closerin   -> 161\n",
      "          white_furtherout -> 329\n",
      "          yellow_closerin  -> 321\n",
      "          yellow_furtherout -> 164\n",
      "[sampler] Group weight multipliers (capped):\n",
      "          other            -> 2.123\n",
      "          white_closerin   -> 2.043\n",
      "          white_furtherout -> 1.000\n",
      "          yellow_closerin  -> 1.025\n",
      "          yellow_furtherout -> 2.006\n",
      "[sampler] weight stats after norm: min=0.687, max=1.458, mean=1.000\n",
      "[sampler] Using inverse-frequency WeightedRandomSampler.\n",
      "[data] ds_train size=1130, ds_val size=276\n",
      "[data] BATCH_SIZE=12, NUM_WORKERS=4\n",
      "\n",
      "[config] Building EfficientNetV2-S model...\n",
      "[init] TorchVision EfficientNetV2-S ImageNet weights\n",
      "[ckpt] loading checkpoint from /scratch/ssriva94/Capstone/new/m-epoch_FL_run3.pth.tar\n",
      "[ckpt] overlay strict=False | missing=674 unexpected=727\n",
      "        missing keys: ['features.0.0.weight', 'features.0.1.weight', 'features.0.1.bias', 'features.0.1.running_mean', 'features.0.1.running_var', 'features.1.0.block.0.0.weight', 'features.1.0.block.0.1.weight', 'features.1.0.block.0.1.bias', 'features.1.0.block.0.1.running_mean', 'features.1.0.block.0.1.running_var']\n",
      "        unexpected keys: ['features.conv0.weight', 'features.norm0.weight', 'features.norm0.bias', 'features.norm0.running_mean', 'features.norm0.running_var', 'features.norm0.num_batches_tracked', 'features.denseblock1.denselayer1.norm1.weight', 'features.denseblock1.denselayer1.norm1.bias', 'features.denseblock1.denselayer1.norm1.running_mean', 'features.denseblock1.denselayer1.norm1.running_var']\n",
      "[ckpt] reinitialized classifier head (MLPHead)\n",
      "[config] Model classifier:\n",
      " MLPHead(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=128, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.5585538051607142, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=14, bias=True)\n",
      "  )\n",
      ")\n",
      "[init] computing class priors from train split...\n",
      "[init] set classifier bias from priors\n",
      "\n",
      "[opt] Using AdamW:\n",
      "      group 0: lr=9.213e-08, weight_decay=3.695e-07\n",
      "      group 1: lr=5.504e-04, weight_decay=3.695e-07\n",
      "[sched] WarmupCosine: warmup=2, epochs=35, min_lr=1e-06\n",
      "\n",
      "[config] LOSS_NAME=asl\n",
      "[loss] Using AsymmetricLossMultiLabel\n",
      "\n",
      "[train] Starting training loop...\n"
     ]
    }
   ],
   "source": [
    "# domain_adapt_from_checkpoint.py\n",
    "# EfficientNetV2-S + MLP head, ASL loss, MixUp/CutMix, TTA, 4-combo sampler.\n",
    "\n",
    "import os, json, random, warnings, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    "    multilabel_confusion_matrix,  # <--- ADDED\n",
    ")\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ========================== PATHS ============================================\n",
    "TRAIN_CSV       = \"/scratch/ssriva94/Capstone/new/low-quality-image.fixed_paths.csv\"\n",
    "TRAIN_PATH_COL  = \"New_path\"\n",
    "\n",
    "TEST_A_CSV      = \"/scratch/ssriva94/Capstone/new/high-quality-image.with_pid.csv\"\n",
    "TEST_A_PATH_COL = \"New_path\"\n",
    "\n",
    "TEST_B_CSV      = \"/scratch/ssriva94/Capstone/new/low-quality-image.fixed_paths.csv\"\n",
    "TEST_B_PATH_COL = \"New_path\"\n",
    "\n",
    "CHECKPOINT_PATH = \"/scratch/ssriva94/Capstone/new/m-epoch_FL_run3.pth.tar\"  # dense checkpoint; strict=False\n",
    "\n",
    "# ============================ LABELS / CONFIG ================================\n",
    "CHEXPERT_LABELS = [\n",
    "    \"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n",
    "    \"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n",
    "    \"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\",\n",
    "]\n",
    "\n",
    "OUTPUT_ROOT = \"./runs_adapt\"\n",
    "\n",
    "# Higher resolution\n",
    "TRAIN_CROP_SIZE = 320\n",
    "VAL_RESIZE      = 384\n",
    "VAL_CROP_SIZE   = 320\n",
    "\n",
    "BATCH_SIZE  = 12\n",
    "VAL_SPLIT   = 0.20\n",
    "SEED        = 1337\n",
    "NUM_WORKERS = 4\n",
    "READ_GRAYSCALE = True\n",
    "THRESH      = 0.5\n",
    "\n",
    "INIT_FROM_IMAGENET           = True\n",
    "OVERLAY_CHECKPOINT_ON_TOP    = True      # will mostly be ignored (different arch), but kept\n",
    "REINIT_CLASSIFIER_AFTER_LOAD = True\n",
    "\n",
    "# ---- Hyperparams (from your good trial) -------------------------------------\n",
    "BACKBONE_LR   = 1.8425919726599874e-07\n",
    "HEAD_LR       = 0.0011007522070230675\n",
    "WEIGHT_DECAY  = 3.69534470108604e-07\n",
    "\n",
    "LOSS_NAME       = \"asl\"       # <--- ASL\n",
    "ACTIVATION_NAME = \"gelu\"\n",
    "HEAD_HIDDEN_DIM = 128\n",
    "HEAD_DROPOUT    = 0.5585538051607142\n",
    "\n",
    "EPOCHS = 35\n",
    "WARMUP_EPOCHS      = 2\n",
    "MIN_LR             = 1e-6\n",
    "EARLYSTOP_PATIENCE = 12\n",
    "\n",
    "# ---- MixUp / CutMix ---------------------------------------------------------\n",
    "USE_MIXUP      = True\n",
    "MIXUP_ALPHA    = 0.2\n",
    "CUTMIX_ALPHA   = 0.2\n",
    "MIXUP_PROB     = 0.7   # prob to apply mixup/cutmix each batch\n",
    "CUTMIX_PROB    = 0.5   # inside that, prob to choose cutmix vs mixup\n",
    "\n",
    "# ---- Test-time augmentation (TTA) -------------------------------------------\n",
    "USE_TTA = True   # simple horizontal flip TTA\n",
    "\n",
    "# ---- Attribute metadata -----------------------------------------------------\n",
    "SUBSET_SOURCE_FILE       = \"/scratch/ssriva94/Capstone/new/low-quality-image.csv\"\n",
    "SUBSET_LIGHTING_COL      = \"Lighting Source\"\n",
    "SUBSET_HEIGHT_COL        = \"Height Variation\"\n",
    "SUBSET_PATH_COL_CANDIDATES = [\"New_path\",\"Path\",\"path\",\"filepath\",\"file_path\"]\n",
    "\n",
    "# all 4 combos for eval\n",
    "SUBSET_DEFINITIONS = [\n",
    "    {\"name\": \"subset_white_furtherout\",\n",
    "     SUBSET_LIGHTING_COL: \"White Light\", SUBSET_HEIGHT_COL: \"Further Out\"},\n",
    "    {\"name\": \"subset_white_closerin\",\n",
    "     SUBSET_LIGHTING_COL: \"White Light\", SUBSET_HEIGHT_COL: \"Closer In\"},\n",
    "    {\"name\": \"subset_yellow_furtherout\",\n",
    "     SUBSET_LIGHTING_COL: \"Yellow Light\", SUBSET_HEIGHT_COL: \"Further Out\"},\n",
    "    {\"name\": \"subset_yellow_closerin\",\n",
    "     SUBSET_LIGHTING_COL: \"Yellow Light\", SUBSET_HEIGHT_COL: \"Closer In\"},\n",
    "]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA requested but not available\")\n",
    "device = torch.device(\"cuda\")\n",
    "AMP = True\n",
    "\n",
    "# =========================== UTILS ===========================================\n",
    "MAP_TRUE  = {\"1\",\"1.0\",\"true\",\"t\",\"yes\",\"y\",\"positive\",\"pos\"}\n",
    "MAP_FALSE = {\"0\",\"0.0\",\"false\",\"f\",\"no\",\"n\",\"negative\",\"neg\"}\n",
    "MAP_UNC   = {\"-1\",\"-1.0\",\"uncertain\",\"u\",\"na\",\"nan\",\"\"}\n",
    "\n",
    "def normalize_label(val):\n",
    "    if pd.isna(val): return 0.0\n",
    "    if isinstance(val,(int,float,np.number)):\n",
    "        return 1.0 if float(val) >= 0.5 else 0.0\n",
    "    s = str(val).strip().strip('\"').strip(\"'\").lower()\n",
    "    if s in MAP_TRUE: return 1.0\n",
    "    if s in MAP_FALSE or s in MAP_UNC: return 0.0\n",
    "    try: return 1.0 if float(s) >= 0.5 else 0.0\n",
    "    except: return 0.0\n",
    "\n",
    "PATIENT_SEG_RE = re.compile(r\"^patient(\\d+)$\", re.IGNORECASE)\n",
    "def pid_from_path_strict_or_fallback(p: str) -> str:\n",
    "    parts = Path(str(p)).parts\n",
    "    for seg in parts:\n",
    "        m = PATIENT_SEG_RE.match(seg)\n",
    "        if m: return f\"patient{m.group(1)}\".lower()\n",
    "    return f\"auto_{Path(str(p)).stem.lower()}\"\n",
    "\n",
    "def ensure_patient_id(df: pd.DataFrame, path_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"patient_id\" not in df.columns:\n",
    "        df[\"patient_id\"] = \"\"\n",
    "    mask = df[\"patient_id\"].astype(str).str.strip().eq(\"\")\n",
    "    if mask.any():\n",
    "        df.loc[mask, \"patient_id\"] = df.loc[mask, path_col].apply(pid_from_path_strict_or_fallback)\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(str).str.lower()\n",
    "    return df\n",
    "\n",
    "def read_img(p):\n",
    "    if READ_GRAYSCALE:\n",
    "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None: raise FileNotFoundError(f\"Missing image: {p}\")\n",
    "        img = np.stack([img,img,img], axis=-1)\n",
    "    else:\n",
    "        img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "        if img is None: raise FileNotFoundError(f\"Missing image: {p}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "class CSVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_col, label_cols, a_transform, expect_labels=True):\n",
    "        df = df.copy()\n",
    "        self.has_labels = expect_labels and all(c in df.columns for c in label_cols)\n",
    "        if self.has_labels:\n",
    "            for c in label_cols:\n",
    "                df[c] = df[c].apply(normalize_label).astype(\"float32\")\n",
    "        exists_mask = df[path_col].astype(str).apply(lambda x: os.path.exists(str(x)))\n",
    "        missing = int((~exists_mask).sum())\n",
    "        if missing:\n",
    "            print(f\"[Dataset] dropping {missing} rows with missing files. Examples:\")\n",
    "            for b in df.loc[~exists_mask, path_col].astype(str).head(10).tolist():\n",
    "                print(\"  -\", b)\n",
    "        self.df = df.loc[exists_mask].reset_index(drop=True)\n",
    "        self.path_col = path_col\n",
    "        self.label_cols = label_cols\n",
    "        self.tf = a_transform\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = read_img(str(row[self.path_col]))\n",
    "        img = self.tf(image=img)[\"image\"]\n",
    "        if self.has_labels:\n",
    "            y = torch.tensor(row[self.label_cols].values.astype(\"float32\"),\n",
    "                             dtype=torch.float32)\n",
    "        else:\n",
    "            y = torch.zeros(len(self.label_cols), dtype=torch.float32)\n",
    "        return img, y\n",
    "\n",
    "def multilabel_acc(probs, target, thr=THRESH):\n",
    "    pred = (probs >= thr).float()\n",
    "    return (pred == target).float().mean(dim=1).mean().item()\n",
    "\n",
    "def _clean_state_dict(sd: dict):\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        kk = k\n",
    "        for pref in (\"module.\",\"densenet121.\",\"efficientnet_v2_s.\",\"model.\",\"backbone.\"):\n",
    "            if kk.startswith(pref): kk = kk[len(pref):]\n",
    "        out[kk] = v\n",
    "    return out\n",
    "\n",
    "# =========================== MODEL HEAD ======================================\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 hidden_dim=None, dropout=0.0, activation_name=\"relu\"):\n",
    "        super().__init__()\n",
    "        act_name = activation_name.lower()\n",
    "        if act_name == \"gelu\":\n",
    "            act = nn.GELU()\n",
    "        elif act_name == \"leaky_relu\":\n",
    "            act = nn.LeakyReLU(inplace=True)\n",
    "        else:\n",
    "            act = nn.ReLU(inplace=True)\n",
    "        layers = []\n",
    "        if hidden_dim and hidden_dim > 0:\n",
    "            layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            layers.append(act)\n",
    "            if dropout and dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            layers.append(nn.Linear(hidden_dim, out_features))\n",
    "        else:\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def build_model():\n",
    "    print(\"\\n[config] Building EfficientNetV2-S model...\")\n",
    "    if INIT_FROM_IMAGENET:\n",
    "        try:\n",
    "            model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "            print(\"[init] TorchVision EfficientNetV2-S ImageNet weights\")\n",
    "        except Exception as e:\n",
    "            print(f\"[init] ImageNet load failed ({e}); using random init\")\n",
    "            model = efficientnet_v2_s(weights=None)\n",
    "    else:\n",
    "        model = efficientnet_v2_s(weights=None)\n",
    "        print(\"[init] Random init\")\n",
    "\n",
    "    # Replace classifier\n",
    "    if isinstance(model.classifier, nn.Sequential):\n",
    "        in_feats = model.classifier[-1].in_features\n",
    "    else:\n",
    "        in_feats = model.classifier.in_features\n",
    "\n",
    "    model.classifier = MLPHead(in_feats, len(CHEXPERT_LABELS),\n",
    "                               hidden_dim=HEAD_HIDDEN_DIM,\n",
    "                               dropout=HEAD_DROPOUT,\n",
    "                               activation_name=ACTIVATION_NAME)\n",
    "\n",
    "    if OVERLAY_CHECKPOINT_ON_TOP and CHECKPOINT_PATH and os.path.isfile(CHECKPOINT_PATH):\n",
    "        try:\n",
    "            print(f\"[ckpt] loading checkpoint from {CHECKPOINT_PATH}\")\n",
    "            ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "            sd = ckpt.get(\"state_dict\", ckpt.get(\"model_state_dict\", ckpt))\n",
    "            if not isinstance(sd, dict): sd = ckpt\n",
    "            sd = _clean_state_dict(sd)\n",
    "            missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "            print(f\"[ckpt] overlay strict=False | missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "            if missing:    print(\"        missing keys:\", missing[:10])\n",
    "            if unexpected: print(\"        unexpected keys:\", unexpected[:10])\n",
    "            if REINIT_CLASSIFIER_AFTER_LOAD:\n",
    "                model.classifier = MLPHead(in_feats, len(CHEXPERT_LABELS),\n",
    "                                           hidden_dim=HEAD_HIDDEN_DIM,\n",
    "                                           dropout=HEAD_DROPOUT,\n",
    "                                           activation_name=ACTIVATION_NAME)\n",
    "                print(\"[ckpt] reinitialized classifier head (MLPHead)\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ckpt] failed to load checkpoint: {e} â€” continuing without it.\")\n",
    "    print(\"[config] Model classifier:\\n\", model.classifier)\n",
    "    return model.to(device)\n",
    "\n",
    "def split_param_groups(model):\n",
    "    head_params, body_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"classifier\" in n: head_params.append(p)\n",
    "        else: body_params.append(p)\n",
    "    return [\n",
    "        {\"params\": body_params, \"lr\": BACKBONE_LR, \"weight_decay\": WEIGHT_DECAY},\n",
    "        {\"params\": head_params,  \"lr\": HEAD_LR,     \"weight_decay\": WEIGHT_DECAY},\n",
    "    ]\n",
    "\n",
    "class WarmupCosine(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, opt, warmup_epochs, total_epochs, min_lr=1e-6, last_epoch=-1):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(opt, last_epoch)\n",
    "    def get_lr(self):\n",
    "        e = self.last_epoch + 1\n",
    "        lrs = []\n",
    "        for base in self.base_lrs:\n",
    "            if e <= self.warmup_epochs:\n",
    "                lrs.append(base * e / max(1, self.warmup_epochs))\n",
    "            else:\n",
    "                t = (e - self.warmup_epochs) / max(1, self.total_epochs - self.warmup_epochs)\n",
    "                lrs.append(self.min_lr + 0.5*(base - self.min_lr)*(1 + np.cos(np.pi*t)))\n",
    "        return lrs\n",
    "\n",
    "# ============================= LOSSES ========================================\n",
    "class AsymmetricLossMultiLabel(nn.Module):\n",
    "    def __init__(self, gamma_pos=0.0, gamma_neg=2.0, clip=0.05,\n",
    "                 eps=1e-8, label_smoothing=0.02, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma_pos = gamma_pos; self.gamma_neg = gamma_neg\n",
    "        self.clip = clip; self.eps = eps\n",
    "        self.ls = label_smoothing; self.reduction = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        if self.ls > 0:\n",
    "            targets = targets * (1 - self.ls) + 0.5 * self.ls\n",
    "        x_sigmoid = torch.sigmoid(logits)\n",
    "        xs_pos = x_sigmoid.clamp(self.eps, 1 - self.eps)\n",
    "        xs_neg = (1.0 - x_sigmoid).clamp(self.eps, 1 - self.eps)\n",
    "        if self.clip and self.clip > 0:\n",
    "            xs_neg = torch.clamp(xs_neg + self.clip, max=1.0)\n",
    "        los_pos = -targets * torch.log(xs_pos)\n",
    "        los_neg = -(1.0 - targets) * torch.log(xs_neg)\n",
    "        if self.gamma_pos > 0 or self.gamma_neg > 0:\n",
    "            with torch.no_grad():\n",
    "                w_pos = (1.0 - xs_pos) ** self.gamma_pos\n",
    "                w_neg = (1.0 - xs_neg) ** self.gamma_neg\n",
    "            los_pos *= w_pos; los_neg *= w_neg\n",
    "        loss = los_pos + los_neg\n",
    "        if self.reduction == \"mean\": return loss.mean()\n",
    "        if self.reduction == \"sum\":  return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# ============================= MIXUP / CUTMIX ================================\n",
    "def rand_bbox(size, lam):\n",
    "    # size: (B, C, H, W)\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def apply_mixup_cutmix(x, y):\n",
    "    if not USE_MIXUP:\n",
    "        return x, y\n",
    "    if random.random() > MIXUP_PROB:\n",
    "        return x, y\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "\n",
    "    use_cutmix = (random.random() < CUTMIX_PROB) and CUTMIX_ALPHA > 0\n",
    "    if use_cutmix:\n",
    "        lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
    "        x1, y1, x2, y2 = rand_bbox(x.size(), lam)\n",
    "        x_cut = x.clone()\n",
    "        x_cut[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n",
    "        # adjust lambda to exactly match pixel ratio\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (x.size(-1) * x.size(-2)))\n",
    "        y_mix = y * lam + y[index] * (1. - lam)\n",
    "        return x_cut, y_mix\n",
    "    else:\n",
    "        # MixUp\n",
    "        if MIXUP_ALPHA <= 0:\n",
    "            return x, y\n",
    "        lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
    "        x_mix = lam * x + (1 - lam) * x[index]\n",
    "        y_mix = lam * y + (1 - lam) * y[index]\n",
    "        return x_mix, y_mix\n",
    "\n",
    "# ============================ EVAL ===========================================\n",
    "def forward_with_tta(model, xb):\n",
    "    if not USE_TTA:\n",
    "        return model(xb)\n",
    "    logits = model(xb)\n",
    "    xb_flip = torch.flip(xb, dims=[3])  # horizontal flip\n",
    "    logits_flip = model(xb_flip)\n",
    "    return 0.5 * (logits + logits_flip)\n",
    "\n",
    "def eval_split(model, loader, split_name, out_dir, labels_present=True):\n",
    "    model.eval()\n",
    "    all_probs, all_true = [], []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=AMP):\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            logits = forward_with_tta(model, xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.append(probs); all_true.append(yb.numpy())\n",
    "    y_prob = np.concatenate(all_probs, axis=0)\n",
    "    y_true = np.concatenate(all_true,  axis=0)\n",
    "\n",
    "    # Save raw probabilities\n",
    "    pd.DataFrame(y_prob, columns=[f\"prob_{c}\" for c in CHEXPERT_LABELS]) \\\n",
    "      .to_csv(os.path.join(out_dir, f\"{split_name}_preds.csv\"), index=False)\n",
    "\n",
    "    if not labels_present:\n",
    "        print(f\"[{split_name}] no labels present. Wrote probabilities.\")\n",
    "        return None\n",
    "\n",
    "    # ---------------- AUROC + ROC curves -------------------------------------\n",
    "    per_class_auroc = []\n",
    "    with PdfPages(os.path.join(out_dir, f\"{split_name}_roc_per_class.pdf\")) as pdf:\n",
    "        for i, name in enumerate(CHEXPERT_LABELS):\n",
    "            y = y_true[:, i]; p = y_prob[:, i]\n",
    "            if len(np.unique(y)) < 2:\n",
    "                per_class_auroc.append(np.nan)\n",
    "                plt.figure(figsize=(6,5))\n",
    "                plt.text(0.5,0.5,\"Insufficient positives/negatives\",\n",
    "                         ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
    "                plt.title(f\"{name} - AUROC: NA\"); pdf.savefig(); plt.close()\n",
    "                continue\n",
    "            au = roc_auc_score(y, p); per_class_auroc.append(au)\n",
    "            fpr, tpr, _ = roc_curve(y, p)\n",
    "            plt.figure(figsize=(6,5))\n",
    "            plt.plot(fpr, tpr, label=\"ROC\"); plt.plot([0,1],[0,1],\"--\")\n",
    "            plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "            plt.title(f\"{name} - AUROC: {au:.3f}\")\n",
    "            plt.legend(); plt.tight_layout(); pdf.savefig(); plt.close()\n",
    "    macro_auroc = float(np.nanmean(per_class_auroc))\n",
    "    df_auroc = pd.DataFrame({\"class\": CHEXPERT_LABELS,\"auroc\":per_class_auroc})\n",
    "    df_auroc = pd.concat([df_auroc,\n",
    "                          pd.DataFrame([{\"class\":\"macro\",\"auroc\":macro_auroc}])],\n",
    "                         ignore_index=True)\n",
    "    df_auroc.to_csv(os.path.join(out_dir, f\"{split_name}_per_class_auroc.csv\"),\n",
    "                    index=False)\n",
    "\n",
    "    # ---------------- PR-AUC --------------------------------------------------\n",
    "    per_class_ap = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        y = y_true[:, i]; p = y_prob[:, i]\n",
    "        if len(np.unique(y)) < 2:\n",
    "            per_class_ap.append(np.nan); continue\n",
    "        per_class_ap.append(average_precision_score(y, p))\n",
    "    macro_ap = float(np.nanmean(per_class_ap))\n",
    "    df_ap = pd.DataFrame({\"class\": CHEXPERT_LABELS,\"prauc\":per_class_ap})\n",
    "    df_ap = pd.concat([df_ap,\n",
    "                       pd.DataFrame([{\"class\":\"macro\",\"prauc\":macro_ap}])],\n",
    "                      ignore_index=True)\n",
    "    df_ap.to_csv(os.path.join(out_dir,f\"{split_name}_per_class_prauc.csv\"),\n",
    "                 index=False)\n",
    "\n",
    "    # ---------------- Confusion matrices (per class) -------------------------\n",
    "    # thresholding at global THRESH\n",
    "    y_pred = (y_prob >= THRESH).astype(int)\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)  # shape: (num_classes, 2, 2)\n",
    "\n",
    "    rows_cm = []\n",
    "    for i, name in enumerate(CHEXPERT_LABELS):\n",
    "        tn, fp, fn, tp = mcm[i].ravel()\n",
    "        rows_cm.append({\n",
    "            \"class\": name,\n",
    "            \"tn\": int(tn),\n",
    "            \"fp\": int(fp),\n",
    "            \"fn\": int(fn),\n",
    "            \"tp\": int(tp),\n",
    "        })\n",
    "    df_cm = pd.DataFrame(rows_cm)\n",
    "    df_cm.to_csv(os.path.join(out_dir, f\"{split_name}_confusion_matrix.csv\"),\n",
    "                 index=False)\n",
    "    print(f\"[{split_name}] Saved per-class confusion matrix CSV.\")\n",
    "\n",
    "    print(f\"[{split_name}] macro AUROC: {macro_auroc:.4f} | macro PR-AUC: {macro_ap:.4f}\")\n",
    "    return macro_auroc\n",
    "\n",
    "def read_table_any(path: str) -> pd.DataFrame:\n",
    "    p = str(path).lower()\n",
    "    if p.endswith(\".xlsx\") or p.endswith(\".xls\"):\n",
    "        return pd.read_excel(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def detect_path_col(df: pd.DataFrame, candidates) -> str:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"No path column found. Looked for: {candidates}\")\n",
    "\n",
    "def _norm_str_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.strip().str.lower()\n",
    "\n",
    "# ============================= MAIN ==========================================\n",
    "def run():\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, run_id); os.makedirs(out_dir, exist_ok=True)\n",
    "    print(\"\\n[run] New run id:\", run_id)\n",
    "    print(\"[run] Output directory:\", out_dir)\n",
    "\n",
    "    with open(os.path.join(out_dir,\"config.json\"),\"w\") as f:\n",
    "        json.dump({\n",
    "            \"TRAIN_CSV\":TRAIN_CSV,\"TRAIN_PATH_COL\":TRAIN_PATH_COL,\n",
    "            \"TEST_A_CSV\":TEST_A_CSV,\"TEST_A_PATH_COL\":TEST_A_PATH_COL,\n",
    "            \"TEST_B_CSV\":TEST_B_CSV,\"TEST_B_PATH_COL\":TEST_B_PATH_COL,\n",
    "            \"CHECKPOINT_PATH\":CHECKPOINT_PATH,\n",
    "            \"BACKBONE_LR\":BACKBONE_LR,\"HEAD_LR\":HEAD_LR,\n",
    "            \"WEIGHT_DECAY\":WEIGHT_DECAY,\"EPOCHS\":EPOCHS,\n",
    "            \"LOSS_NAME\":LOSS_NAME,\"ACTIVATION_NAME\":ACTIVATION_NAME,\n",
    "            \"HEAD_HIDDEN_DIM\":HEAD_HIDDEN_DIM,\"HEAD_DROPOUT\":HEAD_DROPOUT,\n",
    "            \"SUBSET_SOURCE_FILE\":SUBSET_SOURCE_FILE,\n",
    "            \"SUBSET_LIGHTING_COL\":SUBSET_LIGHTING_COL,\n",
    "            \"SUBSET_HEIGHT_COL\":SUBSET_HEIGHT_COL,\n",
    "            \"SUBSET_DEFINITIONS\":SUBSET_DEFINITIONS,\n",
    "            \"USE_MIXUP\":USE_MIXUP,\"USE_TTA\":USE_TTA,\n",
    "            \"TRAIN_CROP_SIZE\":TRAIN_CROP_SIZE,\"VAL_RESIZE\":VAL_RESIZE,\"VAL_CROP_SIZE\":VAL_CROP_SIZE,\n",
    "        }, f, indent=2)\n",
    "    print(\"[run] Saved config.json\")\n",
    "\n",
    "    print(\"\\n[data] Loading training CSV:\", TRAIN_CSV)\n",
    "    df = ensure_patient_id(pd.read_csv(TRAIN_CSV), TRAIN_PATH_COL)\n",
    "    print(\"[data] Total rows (after patient_id ensure):\", len(df))\n",
    "    print(\"[data] Unique patients (total):\", df[\"patient_id\"].nunique())\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=VAL_SPLIT, random_state=SEED)\n",
    "    idx = np.arange(len(df))\n",
    "    tr_idx, va_idx = next(gss.split(idx, groups=df[\"patient_id\"].values))\n",
    "    tr_df = df.iloc[tr_idx].reset_index(drop=True)\n",
    "    va_df = df.iloc[va_idx].reset_index(drop=True)\n",
    "    print(f\"[data] Train rows: {len(tr_df)}, Val rows: {len(va_df)}\")\n",
    "    print(f\"[data] Unique patients (train): {tr_df['patient_id'].nunique()}\")\n",
    "    print(f\"[data] Unique patients (val):   {va_df['patient_id'].nunique()}\")\n",
    "\n",
    "    # Augs\n",
    "    train_transform = A.Compose([\n",
    "        A.RandomResizedCrop(size=(TRAIN_CROP_SIZE, TRAIN_CROP_SIZE),\n",
    "                            scale=(0.8,1.0)),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # mild photometric jitter to help with yellow/lighting variations\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=(0.08,0.18),\n",
    "            contrast_limit=(0.02,0.08),\n",
    "            p=0.3),\n",
    "        A.ColorJitter(brightness=0.05, contrast=0.05,\n",
    "                      saturation=0.05, hue=0.02, p=0.3),\n",
    "        A.GaussNoise(var_limit=(5.0,20.0),mean=0.0,p=0.2),\n",
    "        A.ToFloat(max_value=255.0),\n",
    "        A.Normalize(mean=[0.5]*3,std=[0.5]*3),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    eval_transform = A.Compose([\n",
    "        A.Resize(height=VAL_RESIZE,width=VAL_RESIZE,\n",
    "                 interpolation=cv2.INTER_AREA),\n",
    "        A.CenterCrop(height=VAL_CROP_SIZE,width=VAL_CROP_SIZE),\n",
    "        A.ToFloat(max_value=255.0),\n",
    "        A.Normalize(mean=[0.5]*3,std=[0.5]*3),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    num_workers = min(NUM_WORKERS, os.cpu_count() or 2)\n",
    "\n",
    "    # ------------------ inverse-frequency sampler over 4 combos ---------------\n",
    "    use_attr_sampler = False\n",
    "    sampler = None\n",
    "    tr_df_for_dataset = tr_df\n",
    "\n",
    "    if SUBSET_SOURCE_FILE and os.path.isfile(SUBSET_SOURCE_FILE):\n",
    "        try:\n",
    "            print(\"\\n[sampler] Building inverse-frequency sampler from\",\n",
    "                  SUBSET_SOURCE_FILE)\n",
    "            attr_df = read_table_any(SUBSET_SOURCE_FILE)\n",
    "            subset_path_col = detect_path_col(attr_df, SUBSET_PATH_COL_CANDIDATES)\n",
    "            print(f\"[sampler] Detected path column: {subset_path_col}\")\n",
    "\n",
    "            attr_df[subset_path_col] = attr_df[subset_path_col].astype(str)\n",
    "            df[TRAIN_PATH_COL]       = df[TRAIN_PATH_COL].astype(str)\n",
    "\n",
    "            attr_df[\"_ls\"] = attr_df[SUBSET_LIGHTING_COL].astype(str).str.strip().str.lower()\n",
    "            attr_df[\"_hv\"] = attr_df[SUBSET_HEIGHT_COL].astype(str).str.strip().str.lower()\n",
    "\n",
    "            df_attr_merged = df.merge(\n",
    "                attr_df[[subset_path_col,\"_ls\",\"_hv\"]],\n",
    "                left_on=TRAIN_PATH_COL, right_on=subset_path_col, how=\"left\"\n",
    "            )\n",
    "            tr_df_attr = df_attr_merged.iloc[tr_idx].reset_index(drop=True)\n",
    "\n",
    "            ls = tr_df_attr[\"_ls\"].fillna(\"\").str.lower()\n",
    "            hv = tr_df_attr[\"_hv\"].fillna(\"\").str.lower()\n",
    "\n",
    "            combo_map = {\n",
    "                (\"white light\",\"further out\"): \"white_furtherout\",\n",
    "                (\"white light\",\"closer in\"):   \"white_closerin\",\n",
    "                (\"yellow light\",\"further out\"):\"yellow_furtherout\",\n",
    "                (\"yellow light\",\"closer in\"):  \"yellow_closerin\",\n",
    "            }\n",
    "\n",
    "            groups = []\n",
    "            for l,h in zip(ls.values, hv.values):\n",
    "                groups.append(combo_map.get((l,h),\"other\"))\n",
    "            groups = pd.Series(groups, index=tr_df_attr.index, name=\"group\")\n",
    "\n",
    "            group_counts = groups.value_counts().sort_index()\n",
    "            print(\"[sampler] Group counts in train:\")\n",
    "            for g,c in group_counts.items():\n",
    "                print(f\"          {g:16s} -> {c}\")\n",
    "\n",
    "            max_count = group_counts.max()\n",
    "            max_ratio = 3.0\n",
    "            group_weights = {}\n",
    "            for g,c in group_counts.items():\n",
    "                ratio = max_count / float(c)\n",
    "                ratio = min(ratio, max_ratio)\n",
    "                group_weights[g] = ratio\n",
    "            print(\"[sampler] Group weight multipliers (capped):\")\n",
    "            for g,w in group_weights.items():\n",
    "                print(f\"          {g:16s} -> {w:.3f}\")\n",
    "\n",
    "            weights = np.array([group_weights[g] for g in groups],\n",
    "                               dtype=np.float32)\n",
    "            weights *= (len(weights) / weights.sum())\n",
    "            print(\"[sampler] weight stats after norm: \"\n",
    "                  f\"min={weights.min():.3f}, max={weights.max():.3f}, \"\n",
    "                  f\"mean={weights.mean():.3f}\")\n",
    "\n",
    "            sampler = WeightedRandomSampler(\n",
    "                weights=torch.from_numpy(weights.astype(np.float64)),\n",
    "                num_samples=len(weights),\n",
    "                replacement=True,\n",
    "            )\n",
    "            tr_df_for_dataset = tr_df_attr\n",
    "            use_attr_sampler = True\n",
    "            print(\"[sampler] Using inverse-frequency WeightedRandomSampler.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[sampler] Failed to build sampler: {e}\")\n",
    "            print(\"[sampler] Falling back to standard shuffle.\")\n",
    "    else:\n",
    "        print(f\"[sampler] Attribute file missing or unset: {SUBSET_SOURCE_FILE}\")\n",
    "\n",
    "    ds_train = CSVImageDataset(tr_df_for_dataset, TRAIN_PATH_COL, CHEXPERT_LABELS,\n",
    "                               train_transform, expect_labels=True)\n",
    "    ds_val   = CSVImageDataset(va_df, TRAIN_PATH_COL, CHEXPERT_LABELS,\n",
    "                               eval_transform, expect_labels=True)\n",
    "\n",
    "    if use_attr_sampler and sampler is not None:\n",
    "        dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=num_workers, pin_memory=True)\n",
    "    print(f\"[data] ds_train size={len(ds_train)}, ds_val size={len(ds_val)}\")\n",
    "    print(f\"[data] BATCH_SIZE={BATCH_SIZE}, NUM_WORKERS={num_workers}\")\n",
    "\n",
    "    # ------------------ model / opt / loss -----------------------------------\n",
    "    model = build_model()\n",
    "\n",
    "    # prior-based bias init\n",
    "    try:\n",
    "        print(\"[init] computing class priors from train split...\")\n",
    "        priors = tr_df_for_dataset[CHEXPERT_LABELS].applymap(normalize_label)\\\n",
    "                                                   .mean(axis=0).values.astype(np.float32)\n",
    "        prior_logits = np.log(np.clip(priors,1e-6,1-1e-6) /\n",
    "                              np.clip(1-priors,1e-6,1-1e-6))\n",
    "        last_linear = None\n",
    "        for m in model.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                last_linear = m\n",
    "        if last_linear is not None:\n",
    "            with torch.no_grad():\n",
    "                last_linear.bias.copy_(torch.from_numpy(prior_logits)\n",
    "                                       .to(last_linear.bias.device))\n",
    "            print(\"[init] set classifier bias from priors\")\n",
    "        else:\n",
    "            print(\"[init] WARNING: no Linear layer found in classifier.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[init] could not set classifier bias: {e}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(split_param_groups(model))\n",
    "    scheduler = WarmupCosine(optimizer, WARMUP_EPOCHS, EPOCHS, min_lr=MIN_LR)\n",
    "    print(\"\\n[opt] Using AdamW:\")\n",
    "    for i,g in enumerate(optimizer.param_groups):\n",
    "        print(f\"      group {i}: lr={g['lr']:.3e}, weight_decay={g['weight_decay']:.3e}\")\n",
    "    print(f\"[sched] WarmupCosine: warmup={WARMUP_EPOCHS}, epochs={EPOCHS}, min_lr={MIN_LR}\")\n",
    "\n",
    "    print(f\"\\n[config] LOSS_NAME={LOSS_NAME}\")\n",
    "    criterion = AsymmetricLossMultiLabel()\n",
    "    print(\"[loss] Using AsymmetricLossMultiLabel\")\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "    # ------------------ training loop ----------------------------------------\n",
    "    best_auroc, no_improve = -1.0, 0\n",
    "    hist = {\"epoch\":[], \"lr_head\":[], \"lr_backbone\":[],\n",
    "            \"train_loss\":[], \"train_acc\":[],\n",
    "            \"val_loss\":[], \"val_acc\":[],\n",
    "            \"val_auroc\":[], \"val_prauc\":[]}\n",
    "\n",
    "    print(\"\\n[train] Starting training loop...\")\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tl_sum = ta_sum = 0.0; tn = 0\n",
    "        for xb,yb in dl_train:\n",
    "            xb = xb.to(device,non_blocking=True)\n",
    "            yb = yb.to(device,non_blocking=True)\n",
    "\n",
    "            # MixUp / CutMix\n",
    "            xb_mix, yb_mix = apply_mixup_cutmix(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                logits = model(xb_mix)\n",
    "                loss = criterion(logits, yb_mix)\n",
    "\n",
    "            probs = torch.sigmoid(logits).detach().cpu()\n",
    "            acc = multilabel_acc(probs, yb.detach().cpu(), thr=THRESH)  # acc vs original labels\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "            bs = xb.size(0); tl_sum += float(loss.item())*bs; ta_sum += acc*bs; tn += bs\n",
    "        train_loss = tl_sum/max(1,tn); train_acc = ta_sum/max(1,tn)\n",
    "\n",
    "        model.eval()\n",
    "        vl_sum = va_sum = 0.0; vn = 0\n",
    "        all_probs, all_true = [], []\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=AMP):\n",
    "            for xb,yb in dl_val:\n",
    "                xb = xb.to(device,non_blocking=True)\n",
    "                yb = yb.to(device,non_blocking=True)\n",
    "                logits = forward_with_tta(model, xb)\n",
    "                vloss = criterion(logits, yb)\n",
    "                probs = torch.sigmoid(logits).detach().cpu()\n",
    "                vacc = multilabel_acc(probs, yb.detach().cpu(), thr=THRESH)\n",
    "                all_probs.append(probs.numpy()); all_true.append(yb.cpu().numpy())\n",
    "                bs = xb.size(0); vl_sum += float(vloss.item())*bs; va_sum += vacc*bs; vn += bs\n",
    "        val_loss = vl_sum/max(1,vn); val_acc = va_sum/max(1,vn)\n",
    "\n",
    "        y_true = np.concatenate(all_true,axis=0)\n",
    "        y_prob = np.concatenate(all_probs,axis=0)\n",
    "        per_auroc, per_ap = [], []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            y = y_true[:,i]; p = y_prob[:,i]\n",
    "            if len(np.unique(y))<2:\n",
    "                per_auroc.append(np.nan); per_ap.append(np.nan); continue\n",
    "            per_auroc.append(roc_auc_score(y,p))\n",
    "            per_ap.append(average_precision_score(y,p))\n",
    "        val_macro_auroc = float(np.nanmean(per_auroc))\n",
    "        val_macro_ap    = float(np.nanmean(per_ap))\n",
    "\n",
    "        scheduler.step()\n",
    "        lr_bb = optimizer.param_groups[0][\"lr\"]\n",
    "        lr_hd = optimizer.param_groups[1][\"lr\"]\n",
    "\n",
    "        hist[\"epoch\"].append(epoch)\n",
    "        hist[\"lr_backbone\"].append(lr_bb); hist[\"lr_head\"].append(lr_hd)\n",
    "        hist[\"train_loss\"].append(train_loss); hist[\"train_acc\"].append(train_acc)\n",
    "        hist[\"val_loss\"].append(val_loss);     hist[\"val_acc\"].append(val_acc)\n",
    "        hist[\"val_auroc\"].append(val_macro_auroc); hist[\"val_prauc\"].append(val_macro_ap)\n",
    "\n",
    "        print(f\"[{epoch:02d}/{EPOCHS}] lr_bb={lr_bb:.2e} lr_hd={lr_hd:.2e} | \"\n",
    "              f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
    "              f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} | \"\n",
    "              f\"val_macro_AUROC={val_macro_auroc:.4f} | val_macro_PR-AUC={val_macro_ap:.4f}\")\n",
    "\n",
    "        if val_macro_auroc > best_auroc + 1e-6:\n",
    "            best_auroc, no_improve = val_macro_auroc, 0\n",
    "            torch.save({\"state_dict\":model.state_dict(),\"epoch\":epoch},\n",
    "                       os.path.join(out_dir,\"best.pt\"))\n",
    "            print(f\"[model] Saved new best.pt (epoch={epoch}, macro_AUROC={best_auroc:.4f})\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print(f\"[earlystop] no_improve={no_improve}/{EARLYSTOP_PATIENCE}\")\n",
    "            if no_improve >= EARLYSTOP_PATIENCE:\n",
    "                print(\"[earlystop] stopping early.\")\n",
    "                break\n",
    "\n",
    "    # --------- Final checkpoints ---------------------------------------------\n",
    "    torch.save({\"state_dict\":model.state_dict(),\"epoch\":epoch},\n",
    "           os.path.join(out_dir,\"last.pt\"))\n",
    "    print(f\"[model] Saved last.pt (epoch={epoch})\")\n",
    "    \n",
    "    # --------- Reload BEST checkpoint and export that as .pth ----------------\n",
    "    best_ckpt_path = os.path.join(out_dir, \"best.pt\")\n",
    "    if os.path.isfile(best_ckpt_path):\n",
    "        print(f\"[export] Reloading best model from {best_ckpt_path} for export...\")\n",
    "        ckpt = torch.load(best_ckpt_path, map_location=\"cpu\")\n",
    "        state_dict = ckpt.get(\"state_dict\", ckpt)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print(\"[export] WARNING: best.pt not found, exporting last epoch instead.\")\n",
    "        state_dict = model.state_dict()\n",
    "    \n",
    "    export_path = os.path.join(out_dir, \"model_export.pth\")\n",
    "    torch.save(model.state_dict(), export_path)\n",
    "    print(f\"[export] Saved pure state_dict BEST model to {export_path}\")\n",
    "\n",
    "\n",
    "    # --------- History & plots -----------------------------------------------\n",
    "    hist_df = pd.DataFrame(hist)\n",
    "    hist_df.to_csv(os.path.join(out_dir,\"history.csv\"),index=False)\n",
    "    ep = np.arange(1, len(hist_df)+1)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(ep,hist_df[\"train_loss\"],label=\"train_loss\")\n",
    "    plt.plot(ep,hist_df[\"val_loss\"],label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curves\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_dir,\"train_val_curves.png\")); plt.close()\n",
    "    print(\"[plot] Saved train_val_curves.png\")\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(ep,hist_df[\"val_auroc\"],label=\"val_macro_AUROC\")\n",
    "    plt.plot(ep,hist_df[\"val_prauc\"],label=\"val_macro_PR-AUC\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\")\n",
    "    plt.title(\"Validation AUROC / PR-AUC\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_dir,\"val_auroc_prauc.png\")); plt.close()\n",
    "    print(\"[plot] Saved val_auroc_prauc.png\")\n",
    "\n",
    "    print(\"\\n[val] Per-class metrics on validation split\")\n",
    "    _ = eval_split(model, dl_val, \"val\", out_dir, labels_present=True)\n",
    "\n",
    "    # ---- full test splits ---------------------------------------------------\n",
    "    def maybe_eval(csv_path, path_col, name):\n",
    "        if not csv_path or not os.path.isfile(csv_path):\n",
    "            print(f\"[{name}] skipped (missing file: {csv_path})\"); return\n",
    "        print(f\"\\n[{name}] Evaluating on {csv_path}\")\n",
    "        tdf = ensure_patient_id(pd.read_csv(csv_path), path_col)\n",
    "        ds  = CSVImageDataset(tdf, path_col, CHEXPERT_LABELS,\n",
    "                              eval_transform, expect_labels=True)\n",
    "        dl  = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=num_workers, pin_memory=True)\n",
    "        print(f\"[{name}] Dataset size={len(ds)}\")\n",
    "        _ = eval_split(model, dl, name.lower(), out_dir, labels_present=ds.has_labels)\n",
    "\n",
    "    maybe_eval(TEST_A_CSV, TEST_A_PATH_COL, \"Test A\")\n",
    "    maybe_eval(TEST_B_CSV, TEST_B_PATH_COL, \"Test B\")\n",
    "\n",
    "    # ---- subset evals for all 4 combos -------------------------------------\n",
    "    print(\"\\n[subset] Attribute-based subset evaluation...\")\n",
    "    if SUBSET_SOURCE_FILE and os.path.isfile(SUBSET_SOURCE_FILE):\n",
    "        try:\n",
    "            df_attr = read_table_any(SUBSET_SOURCE_FILE)\n",
    "            print(f\"[subset] Loaded file with {len(df_attr)} rows\")\n",
    "            missing_cols = [c for c in [SUBSET_LIGHTING_COL,SUBSET_HEIGHT_COL]\n",
    "                            if c not in df_attr.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"[subset] missing columns: {missing_cols}\")\n",
    "            else:\n",
    "                subset_path_col = detect_path_col(df_attr, SUBSET_PATH_COL_CANDIDATES)\n",
    "                print(f\"[subset] Detected path column: {subset_path_col}\")\n",
    "                ls_norm = _norm_str_series(df_attr[SUBSET_LIGHTING_COL])\n",
    "                hv_norm = _norm_str_series(df_attr[SUBSET_HEIGHT_COL])\n",
    "                for sd in SUBSET_DEFINITIONS:\n",
    "                    name = sd[\"name\"]\n",
    "                    want_ls = sd[SUBSET_LIGHTING_COL].strip().lower()\n",
    "                    want_hv = sd[SUBSET_HEIGHT_COL].strip().lower()\n",
    "                    mask = ls_norm.eq(want_ls) & hv_norm.eq(want_hv)\n",
    "                    sub = df_attr.loc[mask].copy()\n",
    "                    print(f\"\\n[subset:{name}] target LS='{want_ls}', HV='{want_hv}', \"\n",
    "                          f\"rows before existence check={len(sub)}\")\n",
    "                    if sub.empty:\n",
    "                        print(f\"[subset:{name}] subset empty, skipping.\"); continue\n",
    "                    sub = ensure_patient_id(sub, subset_path_col)\n",
    "                    exists_mask = sub[subset_path_col].astype(str).apply(\n",
    "                        lambda x: os.path.exists(str(x)))\n",
    "                    sub = sub.loc[exists_mask].reset_index(drop=True)\n",
    "                    print(f\"[subset:{name}] rows after existence check={len(sub)}\")\n",
    "                    if sub.empty:\n",
    "                        print(f\"[subset:{name}] no existing files, skipping.\"); continue\n",
    "                    subset_csv_path = os.path.join(out_dir, f\"{name}_rows.csv\")\n",
    "                    sub.to_csv(subset_csv_path,index=False)\n",
    "                    print(f\"[subset:{name}] saved rows to {subset_csv_path}\")\n",
    "                    ds = CSVImageDataset(sub, subset_path_col, CHEXPERT_LABELS,\n",
    "                                         eval_transform, expect_labels=True)\n",
    "                    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                    num_workers=num_workers, pin_memory=True)\n",
    "                    print(f\"[subset:{name}] Evaluating subset (n={len(ds)})\")\n",
    "                    _ = eval_split(model, dl, name, out_dir, labels_present=ds.has_labels)\n",
    "        except Exception as e:\n",
    "            print(f\"[subset] failed subset evaluation: {e}\")\n",
    "    else:\n",
    "        print(f\"[subset] attribute file missing or unset: {SUBSET_SOURCE_FILE}\")\n",
    "\n",
    "    print(f\"\\n[done] All outputs in: {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f686348-e5bd-4096-b809-94d7f27ef2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
